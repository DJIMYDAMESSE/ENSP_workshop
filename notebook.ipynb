{"cells":[{"source":"# Support Vector Machines with Scikit-learn\n**In this tutorial, you'll learn about Support Vector Machines, one of the most popular and widely used supervised machine learning algorithms.**","metadata":{},"id":"congressional-calcium","cell_type":"markdown"},{"source":"SVM offers very high accuracy compared to other classifiers such as logistic regression, and decision trees. It is known for its kernel trick to handle nonlinear input spaces. It is used in a variety of applications such as face detection,  intrusion detection, classification of emails, news articles and web pages, classification of genes, and handwriting recognition.\n\nIn this tutorial, you will be using scikit-learn in Python. If you would like to learn more about this Python package, I recommend you take a look at our [Supervised Learning with scikit-learn](https://www.datacamp.com/courses/supervised-learning-with-scikit-learn) course.\n\nSVM is an exciting algorithm and the concepts are relatively simple. The classifier separates data points using a hyperplane with the largest amount of margin. That's why an SVM classifier is also known as a discriminative classifier.  SVM finds an optimal hyperplane which helps in classifying new data points.  \n\nIn this tutorial, you are going to cover following topics:\n\n* [Support Vector Machines](#support-vector-machines)\n* [How does it work?](#how-does-svm-work)\n* [Kernels](#svm-kernels)\n* [Classifier building in Scikit-learn](#classifier-building-in-scikit-learn)\n* [Tuning Hyperparameters](#tuning-hyperparameters)\n* [Advantages and Disadvantages](#advantages)\n","metadata":{},"id":"animal-mirror","cell_type":"markdown"},{"source":"## Support Vector Machines\n\nGenerally, Support Vector Machines is considered to be a classification approach, but it can be employed in both types of classification and regression problems. It can easily handle multiple continuous and categorical variables. SVM constructs a hyperplane in multidimensional space to separate different classes. SVM generates an optimal hyperplane in an iterative manner, which is used to minimize an error.  The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes.\n\n![](graphics/svm.png)\n\n#### Support Vectors\n\nSupport vectors are the data points, which are closest to the hyperplane. These points will define the separating line better by calculating margins. These points are more relevant to the construction of the classifier.\n\n#### Hyperplane\n\nA hyperplane is a decision plane which separates between a set of objects having different class memberships.\n\n#### Margin\n\nA margin is a gap between the two lines on the closest class points. This is calculated as the perpendicular distance from the line to support vectors or closest points. If the margin is larger in between the classes, then it is considered a good margin, a smaller margin is a bad margin.\n","metadata":{},"id":"9df73888-16fa-4e37-b105-7a520ff705f4","cell_type":"markdown"},{"source":"## SVM Kernels\n\nThe SVM algorithm is implemented in practice using a kernel. A kernel transforms an input data space into the required form. SVM uses a technique called the kernel trick. Here, the kernel takes a low-dimensional input space and transforms it into a higher dimensional space. In other words, you can say that it converts nonseparable problem to separable problems by adding more dimensions to it. It is most useful in non-linear separation problem. Kernel trick helps you to build a more accurate classifier.\n\n* **Linear Kernel**\nA linear kernel can be used as normal dot product any two given observations. The product between two vectors is the sum of the multiplication of each pair of input values.\n\n```\nK(x, xi) = sum(x * xi)\n```\n\n* **Polynomial Kernel**\nA polynomial kernel is a more generalized form of the linear kernel. The polynomial kernel can distinguish curved or nonlinear input space.\n\n```\nK(x,xi) = 1 + sum(x * xi)^d\n```\n\nWhere d is the degree of the polynomial. d=1 is similar to the linear transformation. The degree needs to be manually specified in the learning algorithm.\n\n* **Radial Basis Function Kernel**\nThe Radial basis function kernel is a popular kernel function commonly used in support vector machine classification. RBF can map an input space in infinite dimensional space.\n\n```\nK(x,xi) = exp(-gamma * sum((x – xi^2))\n```","metadata":{},"id":"1b8400be-2e39-4de2-8b95-e7d976742d32","cell_type":"markdown"},{"source":"\nHere gamma is a parameter, which ranges from 0 to 1. A higher value of gamma will perfectly fit the training dataset, which causes over-fitting. Gamma=0.1 is considered to be a good default value. The value of gamma needs to be manually specified in the learning algorithm.","metadata":{},"id":"effective-ocean","cell_type":"markdown"},{"source":"## Classifier Building in Scikit-learn\n\nUntil now, you have learned about the theoretical background of SVM. Now you will learn about its implementation in  Python using scikit-learn.\n\nIn the model the building part, you can use the cancer dataset, which is a very famous multi-class classification problem. This dataset is computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n\nThe dataset comprises \n- 30 **features** (mean radius, mean texture, mean perimeter, mean area, mean smoothness, mean compactness, mean concavity, mean concave points, mean symmetry, mean fractal dimension, radius error, texture error, perimeter error, area error, smoothness error, compactness error, concavity error, concave points error, symmetry error, fractal dimension error, worst radius, worst texture, worst perimeter, worst area, worst smoothness, worst compactness, worst concavity, worst concave points, worst symmetry, and worst fractal dimension) \n- and a **target** (type of cancer).\n\nThis data has two types of cancer classes: malignant (harmful) and benign (not harmful). Here, you can build a model to classify the type of cancer. The dataset is available in the scikit-learn library or you can also download it from the UCI Machine Learning Library.\n\n#### Loading Data\n\nLet's first load the required dataset you will use.\n","metadata":{},"id":"individual-corpus","cell_type":"markdown"},{"source":"# !pip install -r requirements.txt","metadata":{},"id":"excellent-percentage","cell_type":"code","execution_count":1,"outputs":[]},{"source":"#Import scikit-learn dataset library\nfrom sklearn import datasets\n\n#Load dataset\ncancer = datasets.load_breast_cancer()","metadata":{},"id":"sporting-landscape","cell_type":"code","execution_count":2,"outputs":[]},{"source":"#### Exploring Data\n\nAfter you have loaded the dataset, you might want to know a little bit more about it. You can check feature and target names.  \n","metadata":{},"id":"every-spell","cell_type":"markdown"},{"source":"# print the names of the 13 features\nprint(\"Features: \", cancer.feature_names)\n\n# print the label type of cancer('malignant' 'benign')\nprint(\"Labels: \", cancer.target_names)","metadata":{"outputsMetadata":{"0":{"height":397,"type":"stream"}}},"id":"working-angle","cell_type":"code","execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":["Features:  ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n"," 'mean smoothness' 'mean compactness' 'mean concavity'\n"," 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n"," 'radius error' 'texture error' 'perimeter error' 'area error'\n"," 'smoothness error' 'compactness error' 'concavity error'\n"," 'concave points error' 'symmetry error' 'fractal dimension error'\n"," 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n"," 'worst smoothness' 'worst compactness' 'worst concavity'\n"," 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n","Labels:  ['malignant' 'benign']\n"]}]},{"source":"Let's explore it for a bit more. You can also check the shape of the dataset using shape.\n","metadata":{},"id":"plastic-little","cell_type":"markdown"},{"source":"# print data(feature)shape\ncancer.data.shape","metadata":{"lines_to_next_cell":0},"id":"serious-caution","cell_type":"code","execution_count":4,"outputs":[{"data":{"text/plain":["(569, 30)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}]},{"source":"Let's check top 5 records of the feature set.\n","metadata":{},"id":"introductory-tomato","cell_type":"markdown"},{"source":"# print the cancer data features (top 5 records)\nprint(cancer.data[0:5])","metadata":{"outputsMetadata":{"0":{"height":516,"type":"stream"}}},"id":"reflected-transformation","cell_type":"code","execution_count":5,"outputs":[{"name":"stdout","output_type":"stream","text":["[[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n","  1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n","  6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n","  1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n","  4.601e-01 1.189e-01]\n"," [2.057e+01 1.777e+01 1.329e+02 1.326e+03 8.474e-02 7.864e-02 8.690e-02\n","  7.017e-02 1.812e-01 5.667e-02 5.435e-01 7.339e-01 3.398e+00 7.408e+01\n","  5.225e-03 1.308e-02 1.860e-02 1.340e-02 1.389e-02 3.532e-03 2.499e+01\n","  2.341e+01 1.588e+02 1.956e+03 1.238e-01 1.866e-01 2.416e-01 1.860e-01\n","  2.750e-01 8.902e-02]\n"," [1.969e+01 2.125e+01 1.300e+02 1.203e+03 1.096e-01 1.599e-01 1.974e-01\n","  1.279e-01 2.069e-01 5.999e-02 7.456e-01 7.869e-01 4.585e+00 9.403e+01\n","  6.150e-03 4.006e-02 3.832e-02 2.058e-02 2.250e-02 4.571e-03 2.357e+01\n","  2.553e+01 1.525e+02 1.709e+03 1.444e-01 4.245e-01 4.504e-01 2.430e-01\n","  3.613e-01 8.758e-02]\n"," [1.142e+01 2.038e+01 7.758e+01 3.861e+02 1.425e-01 2.839e-01 2.414e-01\n","  1.052e-01 2.597e-01 9.744e-02 4.956e-01 1.156e+00 3.445e+00 2.723e+01\n","  9.110e-03 7.458e-02 5.661e-02 1.867e-02 5.963e-02 9.208e-03 1.491e+01\n","  2.650e+01 9.887e+01 5.677e+02 2.098e-01 8.663e-01 6.869e-01 2.575e-01\n","  6.638e-01 1.730e-01]\n"," [2.029e+01 1.434e+01 1.351e+02 1.297e+03 1.003e-01 1.328e-01 1.980e-01\n","  1.043e-01 1.809e-01 5.883e-02 7.572e-01 7.813e-01 5.438e+00 9.444e+01\n","  1.149e-02 2.461e-02 5.688e-02 1.885e-02 1.756e-02 5.115e-03 2.254e+01\n","  1.667e+01 1.522e+02 1.575e+03 1.374e-01 2.050e-01 4.000e-01 1.625e-01\n","  2.364e-01 7.678e-02]]\n"]}]},{"source":"Let's take a look at the target set.\n","metadata":{},"id":"incorporate-johnston","cell_type":"markdown"},{"source":"# print the cancer labels (0:malignant, 1:benign)\nprint(cancer.target)","metadata":{"outputsMetadata":{"0":{"height":516,"type":"stream"}}},"id":"nuclear-westminster","cell_type":"code","execution_count":6,"outputs":[{"name":"stdout","output_type":"stream","text":["[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n"," 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n"," 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n"," 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n"," 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n"," 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n"," 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n"," 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n"," 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n"," 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n"," 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n"," 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"]}]},{"source":"#### Splitting Data\n\nTo understand model performance, dividing the dataset into a training set and a test set is a good strategy.\n\nSplit the dataset by using the function `train_test_split()`. you need to pass 3 parameters features, target, and test_set size. Additionally, you can use random_state to select records randomly.\n","metadata":{},"id":"final-daniel","cell_type":"markdown"},{"source":"# Import train_test_split function\nfrom sklearn.model_selection import train_test_split\n\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3,random_state=109) # 70% training and 30% test","metadata":{},"id":"female-january","cell_type":"code","execution_count":7,"outputs":[]},{"source":"#### Generating Model\n\nLet's build support vector machine model. First, import the SVM module and create support vector classifier object by passing argument kernel as the linear kernel in `SVC()` function.\n\nThen, fit your model on train set using `fit()` and perform prediction on the test set using `predict()`.\n","metadata":{},"id":"exact-uncle","cell_type":"markdown"},{"source":"#Import svm model\nfrom sklearn import svm\n\n#Create a svm Classifier\nclf = svm.SVC(kernel='linear') # Linear Kernel\n\n#Train the model using the training sets\nclf.fit(X_train, y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)","metadata":{},"id":"handed-registration","cell_type":"code","execution_count":8,"outputs":[]},{"source":"#### Evaluating the Model\n\nLet's estimate how accurately the classifier or model can predict the breast cancer of patients.\n\nAccuracy can be computed by comparing actual test set values and predicted values.\n","metadata":{},"id":"mediterranean-sudan","cell_type":"markdown"},{"source":"#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# Model Accuracy: how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","metadata":{"outputsMetadata":{"0":{"height":37,"type":"stream"}}},"id":"accessible-regulation","cell_type":"code","execution_count":9,"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.9649122807017544\n"]}]},{"source":"Well, you got a classification rate of 96.49%, considered as very good accuracy.\n\nFor further evaluation, you can also check precision and recall of model.\n","metadata":{},"id":"large-practice","cell_type":"markdown"},{"source":"# Model Precision: what percentage of positive tuples are labeled as such?\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\n\n# Model Recall: what percentage of positive tuples are labelled as such?\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))","metadata":{"outputsMetadata":{"0":{"height":77,"type":"stream"}}},"id":"israeli-importance","cell_type":"code","execution_count":10,"outputs":[{"name":"stdout","output_type":"stream","text":["Precision: 0.9811320754716981\n","Recall: 0.9629629629629629\n"]}]},{"source":"Well, you got a precision of 98% and recall of 96%, which are considered as very good values.","metadata":{},"id":"f4d6337d-2858-44f1-95c3-590d41f17533","cell_type":"markdown"},{"source":"Well, you got a precision of 98% and recall of 96%, which are considered as very good values.\n\n## Tuning Hyperparameters\n\n* **Kernel**: The main function of the kernel is to transform the given dataset input data into the required form. There are various types of functions such as linear, polynomial, and radial basis function (RBF). Polynomial and RBF are useful for non-linear hyperplane. Polynomial and RBF kernels compute the separation line in the higher dimension. In some of the applications, it is suggested to use a more complex kernel to separate the classes that are curved or nonlinear. This transformation can lead to more accurate classifiers.\n* **Regularization**: Regularization parameter in python's Scikit-learn C parameter used to maintain regularization. Here C is the penalty parameter, which represents misclassification or error term. The misclassification or error term tells the SVM optimization how much error is bearable. This is how you can control the trade-off between decision boundary and misclassification term. A smaller value of C creates a small-margin hyperplane and a larger value of C creates a larger-margin hyperplane.\n* **Gamma**: A lower value of Gamma will loosely fit the training dataset, whereas a higher value of gamma will exactly fit the training dataset, which causes over-fitting. In other words, you can say a low value of gamma considers only nearby points in calculating the separation line, while the a value of gamma considers all the data points in the calculation of the separation line.","metadata":{},"id":"comic-evening","cell_type":"markdown"},{"source":"## Advantages\n\nSVM Classifiers offer good accuracy and perform faster prediction compared to Naïve Bayes algorithm. They also use less memory because they use a subset of training points in the decision phase. SVM works well with a clear margin of separation and with high dimensional space.  \n\n## Disadvantages","metadata":{},"id":"colonial-vessel","cell_type":"markdown"},{"source":"SVM is not suitable for large datasets because of its high training time and it also takes more time in training compared to Naïve Bayes. It works poorly with overlapping classes and is also sensitive to the type of kernel used.\n\n## Conclusion\n\nCongratulations, you have made it to the end of this tutorial!\n\nIn this tutorial, you covered a lot of ground about Support vector machine algorithm, its working, kernels, hyperparameter tuning, model building and evaluation on breast cancer dataset using the Scikit-learn package. You have also covered its advantages and disadvantages. I hope you have learned something valuable!\n\nTo learn more about this type of classifiers, you should take a look at our [Linear Classifiers in Python](https://www.datacamp.com/courses/linear-classifiers-in-python) course. It introduces other types of regression and loss functions, as well as Support Vector Machines.\n\nI look forward to hearing any feedback or questions. You can ask the question by leaving a comment and I will try my best to answer it.","metadata":{},"id":"compliant-progress","cell_type":"markdown"}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}