{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5688b4e8",
   "metadata": {
    "id": "5688b4e8"
   },
   "source": [
    "# Tutorial 4: Classification using machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd0a18",
   "metadata": {
    "id": "97dd0a18"
   },
   "source": [
    "#### Hello, and welcome to the final tutorial in this series!\n",
    "\n",
    "This tutorial will show you how to:\n",
    " - Use different machine learning methods (logistic regression and (later) random forest) for classification\n",
    " - Use a pipeline to organize your training and testing.\n",
    " - Apply hyperparameter optimization to improve the classifiers\n",
    " - Compare those methods against each other and a baseline using jackknife\n",
    " \n",
    " Can't wait to get started! First as usual we download the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a00d72",
   "metadata": {
    "id": "33a00d72"
   },
   "outputs": [],
   "source": [
    "# ___Cell no. 1___\n",
    "\n",
    "## Python packages \n",
    "from sklearn.linear_model import LogisticRegression # a ML method\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23645e6-3ef7-4ed0-b06a-3167688c416a",
   "metadata": {
    "id": "d23645e6-3ef7-4ed0-b06a-3167688c416a"
   },
   "source": [
    "We also need the data stored in files `df` and `base_dict` which were created in Tutorial 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4635a-5b15-4533-934e-40f031480085",
   "metadata": {
    "id": "14e4635a-5b15-4533-934e-40f031480085"
   },
   "outputs": [],
   "source": [
    "# ___Cell no. 2___\n",
    "\n",
    "%store -r df\n",
    "%store -r base_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceba4424",
   "metadata": {
    "id": "ceba4424"
   },
   "source": [
    "### Set up machine learning methods and their hyperparameters\n",
    "\n",
    "Logistic regression is a leading machine learning (ML) method  (later on we will use random forest, which is another common method). We will compare the results from multiple classifiers. First we create aliases for the classifiers, and create a list to use when we apply the classifiers one by one.\n",
    "\n",
    "For our initial trial we will use two variants of logistic regression:  with or without fitting an intercept.  Basically, fitting an intercept means that the line (or plane) that separates between the two classes can be any line, while not fitting an intercept means that the line of separation must pass through the origin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4be85a",
   "metadata": {
    "id": "7c4be85a"
   },
   "outputs": [],
   "source": [
    "# ___Cell no. 3___\n",
    "\n",
    "lr = LogisticRegression() # defining the model\n",
    "\n",
    "## Define a list of models, We will use this when we loop through ML methods.\n",
    "### Here we define two variants of logistic regression: one which fits an intercept, and one that does not.\n",
    "### (basically, fitting an intercept means that the line of separation between the two classes can be any line,\n",
    "### while not fitting an intercept means that the line of separation must pass through the origin)\n",
    "models = [ [lr, 'lrFI'], [lr, 'lrNFI']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d8750d",
   "metadata": {
    "id": "b2d8750d"
   },
   "source": [
    "ML methods have *hyperparameters* which must be defined before they can run. The hyperparameter values used can significantly affect the performance of the methods. hyperparameters specify the _process_ of fitting: they do not appear in the final data model, but they determine how the data model is computed. For this reason, *hyperparameter optimization* is used to find the best possible values for these hyperparameters. \n",
    "Basically, this is done using trial and error: each ML algorithm is trained with multiple combinations of hyperparameters, and the best combination is chosen (this is called *grid search*).\n",
    " \n",
    "Each method has its own set of hyperparameters  In the following code block we set possible values for hyperparameters for logistic regression (LR). For example, we are using an L2 penalty. Another possible penalty is L1. To understand the difference see:\n",
    "\n",
    "https://builtin.com/data-science/l2-regularization\n",
    "\n",
    "Essentially, L2 penalty tends to equalize more the sizes of the regression coefficients, while L1 tends to favor more the coefficients for those features that are better predictors.\n",
    "\n",
    "(It turns out that L1 doesn't work well on our dataset, so we will stick with L2.)\n",
    "\n",
    "For more information on other hyperparameters for LR, as well as a more complete description of grid search check the following links:\n",
    "\n",
    "For logistic regression hyperparameters:\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "For grid search:\n",
    "- https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/\n",
    "\n",
    "Other methods have other parameter sets. For example, the parameter sets for Random Forest (which we will use later) are here:\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html (RF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c0f44",
   "metadata": {
    "id": "0a3c0f44"
   },
   "outputs": [],
   "source": [
    "# ___Cell no. 4___\n",
    "\n",
    "## logistic regression (LR) with intercept fitting\n",
    "solvers = ['newton-cg', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [16.0,4.0, 1.0, 0.25]\n",
    "lrFI_par = dict(solver=solvers,penalty=penalty,C=c_values) # creating a dictionary that contains the hyperparamters\n",
    "\n",
    "## @@@ Initialize parameters for LR without intercept and put in a dictionary named `lrNFI_par`. All parameters are the same, except set  the `fit_intercept` \n",
    "## @@@  option as False.  See the above documentation for LR.\n",
    "\n",
    "## Define a list of parameter sets to go along with the list of models.\n",
    "parameters = [lrFI_par, lrNFI_par]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d8203",
   "metadata": {
    "id": "0e9d8203"
   },
   "source": [
    "To set up the training and testing sets, we need to specify inputs (feature sets), outputs (targets), and the training/testing split. We will apply each ML method to three different feature sets, and compare the performances. The code is very similar to the code for the baseline estimator (Mahalanobis distance). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37732fb8",
   "metadata": {
    "id": "37732fb8"
   },
   "outputs": [],
   "source": [
    "# ___Cell no. 5___\n",
    "\n",
    "y = df[['type']] # define the targets\n",
    "features = [ ['TPC_H2O','FRAP_H2O'], ['TEAC_H2O','FRAP_H2O'], ['TPC_H2O', 'TEAC_H2O']] # define a list of features\n",
    "splits = 0.4 # definplit (Feel free to replace with a better value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a8d6a-a063-46f5-999a-0b6f1ae06c14",
   "metadata": {
    "id": "cd5a8d6a-a063-46f5-999a-0b6f1ae06c14"
   },
   "source": [
    "#### Training\n",
    "\n",
    "Just like the previous tutorial, the following code trains and evaluates multiple classifiers (3 feature sets with 2 ML models). For this purpose, we define a function `get_accuracy_ml` which produces the following outputs for each classifier:\n",
    "\n",
    "* `tot_acc`    : total accuracy;\n",
    "* `jack_trainSD` : jackknife estimate of standard deviation of accuracy due to training errors \n",
    "* `jack_testSD`  : jackknife estimate of standard deviation of accuracy due to testing error\n",
    "* `jack_totSD`   : jackknife estimate of standard deviation of accuracy estimate.\n",
    "\n",
    "The function `get_accuracy_ml` also requires another function `cv` that does _cross-validation_. A good reference for this is Jason Brownlee's \"Machine Learning Mastery\" blog:\n",
    "\n",
    "https://machinelearningmastery.com/k-fold-cross-validation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a175f-e598-4019-9762-442de9343c22",
   "metadata": {
    "id": "b95a175f-e598-4019-9762-442de9343c22"
   },
   "outputs": [],
   "source": [
    "# ___Cell no. 6___\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import *\n",
    "from sklearn import metrics\n",
    "\n",
    "def cv (m, p, xtrain, ytrain, xtest, ytest):\n",
    "    inner_cv = StratifiedKFold(n_splits=3)\n",
    "    # The following statement optimizes the model m among parameter options p, making use of the given training and testing set\n",
    "    clf = GridSearchCV(m, p, scoring='accuracy', n_jobs=-1, cv=inner_cv, refit=True, verbose=0)\n",
    "    clf.fit( xtrain,  ytrain)\n",
    "    yPred = clf.predict(xtest)# This is the prediction derived from the optimized model on the testing set\n",
    "    #@@@ Set 'result' equal to the accuracy calculated by comparing the predictions (yPred) and the true labels (ytrain)\n",
    "    return result,clf\n",
    " \n",
    "\n",
    "def get_accuracy_ml (m, p, xTrain, yTrain, xTest, yTest):\n",
    "    accTot,clfTot = cv(m, p, xTrain, yTrain, xTest, yTest)\n",
    "    \n",
    "    # Compute jackknife estimates\n",
    "    # Need number of estimates to define lists of results\n",
    "    nxtr = len(yTrain)\n",
    "    nxte=len(yTest)\n",
    "    \n",
    "    # Make space for lists of leave-out-one results\n",
    "    # Use jackknife to estimate variation due to training error\n",
    "    jackTrainArr = np.zeros(nxtr)\n",
    "            \n",
    "    for i in range(len(xTrain)):\n",
    "        x_train = np.delete(np.array(xTrain), i, 0)\n",
    "        y_train = np.delete(np.array(yTrain), i, 0)\n",
    "        \n",
    "        scoreTrain,clfTrain = cv(m, p, x_train, y_train, xTest, yTest)\n",
    "        jackTrainArr[i] = scoreTrain\n",
    "            \n",
    "    # @@@ write a similar code to create jackTestArr\n",
    "    \n",
    "    # Compute SD's for error due to training and testing via jackknife formula\n",
    "    # The total SD is computed using the variance addition formula\n",
    "    jackTrainSD = # @@@ Complete this (similar to the code used for Mahalanobis)\n",
    "    jackTestSD = # @@@ Complete this\n",
    "    jackTotSD = np.sqrt(jackTrainSD**2 + jackTestSD**2)\n",
    "    # Finished! return results\n",
    "    return  accTot, jackTrainSD,jackTestSD,jackTotSD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be0d1ea-975f-4244-b343-2142d693591a",
   "metadata": {
    "id": "4be0d1ea-975f-4244-b343-2142d693591a"
   },
   "source": [
    "Next we define an empty dictionary to hold our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13c6dc-2c57-4396-b7f1-59abf64dd2c3",
   "metadata": {
    "id": "8d13c6dc-2c57-4396-b7f1-59abf64dd2c3"
   },
   "outputs": [],
   "source": [
    "# ___Cell no. 7___\n",
    "\n",
    "ml_dicts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47954618",
   "metadata": {
    "id": "47954618"
   },
   "outputs": [],
   "source": [
    "# ___Cell no. 8___\n",
    "\n",
    "# Loop through different ML models and hyper parameters combinations (use the same splits for all features)\n",
    "\n",
    "for m, par in zip(models, parameters):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, y, test_size= splits, random_state=1, stratify = y, shuffle = True)\n",
    "    \n",
    "    # defining The main subkeys, which are the machine learning models\n",
    "    key0 = str(m[1])\n",
    "    print(\"Main key for the following iterations:\")\n",
    "    print(key0)\n",
    "    \n",
    "    # Define sub-dictionaries for each main key to hold results for different feature combinations\n",
    "    ml_dicts[key0] = {} \n",
    "    \n",
    "    # Loop through features\n",
    "    for f in features:\n",
    "        xtr =  X_train[f] \n",
    "        xte =  X_test[f]\n",
    "        # Use `get_accuracy_ml` to obtain results for given classifier \n",
    "        results = get_accuracy_ml (m[0], par, np.array(xtr), np.array(y_train).flatten(), np.array(xte), np.array(y_test).flatten()) # to get the accuracies for the ml model\n",
    "        \n",
    "        key = str(splits)+\",\"+str((f)) # Create keys for the each feature set in order to reference results\n",
    "        # Create subdictionary for each feature combinations to hold results for that specific combo. \n",
    "        ml_dicts[key0][key] = {}\n",
    "\n",
    "        # Put results into dictionary\n",
    "        ml_dicts[key0][key]['tot_acc'] = results[0]\n",
    "        # @@@ similarly, put the training, testing and total standard deviations in the dictionary \n",
    "        # @@@ (You may compare the code used for Mahalanobis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cfd63d",
   "metadata": {
    "id": "34cfd63d"
   },
   "source": [
    "Let's take a look at how the outputs are organized. We'll print some of the dictionary keys in the output data structure, which is called 'ml_dicts'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e9fd7",
   "metadata": {
    "id": "d52e9fd7"
   },
   "outputs": [],
   "source": [
    "# ___Cell no. 9___\n",
    "\n",
    "print(\"The main keys are: \"+ str(ml_dicts.keys()) + \" which stands for the main ML models\\n\")\n",
    "print(\"The main subkeys for the first estimator are:\")\n",
    "print(str(ml_dicts['lr'].keys()) + \" which stands for the main ML features\\n\")\n",
    "print(\"The subkeys inside the first feature set:\")\n",
    "# @@@ Print out the subkeys for the first features set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64128103",
   "metadata": {
    "id": "64128103"
   },
   "source": [
    "**Exercise:** Display the same results for the second estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a237bc-8a6f-4a6a-be7a-b46f0f9561fa",
   "metadata": {
    "id": "66a237bc-8a6f-4a6a-be7a-b46f0f9561fa"
   },
   "outputs": [],
   "source": [
    "#  ___ code here ____\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaeef0f-92d3-43db-b3ff-34f780074d8b",
   "metadata": {
    "id": "bbaeef0f-92d3-43db-b3ff-34f780074d8b"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bcb5d3",
   "metadata": {
    "id": "37bcb5d3"
   },
   "source": [
    "#### Create arrays for results\n",
    "\n",
    "As in Tutorial 3, we form arrays so that we can do a plot with error bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12552e5d",
   "metadata": {
    "id": "12552e5d"
   },
   "outputs": [],
   "source": [
    "# ___Cell no. 10___\n",
    "\n",
    "arr_all = []\n",
    "for m, d in zip (models, ml_dicts.keys()):\n",
    "    acc_arr = [] \n",
    "    sd_arr = [] \n",
    "\n",
    "    # @@@ Write code to put the accuracies and total standard deviations for all models and feature sets \n",
    "    # @@@ in the arrays `acc_arr` and `sd_arr`, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632aa9a7",
   "metadata": {
    "id": "632aa9a7"
   },
   "source": [
    "Just to double check we're OK, we print the results. (We will graph them in a minute.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0682d53-ef78-44f3-8e2b-d00ebd181ea1",
   "metadata": {
    "id": "a0682d53-ef78-44f3-8e2b-d00ebd181ea1"
   },
   "outputs": [],
   "source": [
    "# ___Cell no. 11___\n",
    "\n",
    "print(arr_all) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73744243-a407-448d-a79e-5ff9d3afb2ef",
   "metadata": {
    "id": "73744243-a407-448d-a79e-5ff9d3afb2ef"
   },
   "source": [
    "#### Graphing the results\n",
    "By graphing the results and error bars for each feature set and method, we make it easy to compare accuracies across the different feature sets and ML methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217374ce-fd4b-4740-a7b5-e3dbac8ff3af",
   "metadata": {
    "id": "217374ce-fd4b-4740-a7b5-e3dbac8ff3af"
   },
   "outputs": [],
   "source": [
    "# ___Cell no. 12___\n",
    "\n",
    "colors = ['blue', 'purple'] # colors for the different methods\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.title( \"Accuracy  for different features with the SD\", fontweight ='bold', fontsize =12)\n",
    "plt.xlabel(\"Features\", fontweight ='bold', fontsize =12)\n",
    "plt.ylabel(\"Accuracy\", fontweight ='bold', fontsize =12)\n",
    "\n",
    "count = 0\n",
    "n = 5\n",
    "\n",
    "space = []\n",
    "tickFeat = []\n",
    "\n",
    "for result, model, color in zip(arr_all, models, colors):\n",
    "    a = np.linspace(n*count, n*(1+count)-2,len(features)) # to get index on the x-axis\n",
    "    space.extend(a)\n",
    "    tickFeat.extend(result[0])\n",
    "    plt.errorbar( a, result[1], result[2], fmt='o', label =model[1], color = color)\n",
    "    count += 1\n",
    "\n",
    "# @@@ Label the ticks on the x-axis with the feature set names\n",
    "# @@@ Set reasonable y limits for your plot\n",
    "# @@@ Add a legend that indicates the two types of LR model used.\n",
    "# @@@ Show your plot using plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22295e",
   "metadata": {
    "id": "da22295e"
   },
   "source": [
    "**Exercise:** Summarize your results. Which estimator appears to be most accurate? Which is the most stable (in terms of small error bars)? What about statistical significance? Does including intercept fitting provide any advantage?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5cf9a",
   "metadata": {
    "id": "6ee5cf9a"
   },
   "source": [
    "### Displaying performance relative to the baseline\n",
    "\n",
    "Do these classifiers really give us an advantage over a very simple statistical classifier that is based on Mahalanobis distance? We choose as our baseline the best Mahalanobis classifier that we found in the previous tutorial, and we plot the accuracy difference between our ML classifers and this baseline (with error bars).\n",
    "\n",
    "First we create a list of relative accuracies and standard deviations for the different ML classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acba26e",
   "metadata": {
    "id": "4acba26e"
   },
   "outputs": [],
   "source": [
    "# ___Cell no. 13___\n",
    "\n",
    "# List containing data for different classifiers\n",
    "arr_diff_all = []\n",
    "\n",
    "acc_base = base_dict[\"0.4\"][\"['TPC_H2O', 'TEAC_H2O']\"][ 'tot_acc' ]\n",
    "# @@@ Define `var_base` as the variance of the base estimator (square of standard deviation)\n",
    "\n",
    "# Loop through models to complie all data\n",
    "for m, m_key in zip (models, ml_dicts.keys()):\n",
    "    acc_diff_arr = [] \n",
    "    sd_diff_arr = [] \n",
    "    for f_key in ml_dicts[m_key].keys():\n",
    "        \n",
    "        accDiff = ml_dicts[m_key][f_key][ 'tot_acc' ] -  acc_base\n",
    "        acc_diff_arr.append(accDiff )\n",
    "        # @@@ Obtain the standard deviation of the current model, call it 'sd'\n",
    "        \n",
    "        # @@@ Append to `sd_diff_arr` the standard deviation of the difference, which is the square root of the \n",
    "        # @@@ sum of base variance  + variance of current model  \n",
    "        # @@@ (the variance of the current model is the square of sd)       \n",
    "    arr_diff_all.append([ list(ml_dicts[m_key].keys()), acc_diff_arr, sd_diff_arr])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1855b66",
   "metadata": {
    "id": "e1855b66"
   },
   "source": [
    "The information is stored in a list that tells the train/test split and features; mean accuracy difference; and standard deviation of accuracy for the baseline and ML classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ac6cbc",
   "metadata": {
    "id": "82ac6cbc"
   },
   "outputs": [],
   "source": [
    "# ___Cell no. 14___\n",
    "\n",
    "print(arr_diff_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bd01c2-4191-42b9-8a29-6da6e05a7057",
   "metadata": {
    "id": "46bd01c2-4191-42b9-8a29-6da6e05a7057"
   },
   "source": [
    "Since the results are arranged in an orderly fashion, we can now graph them. Since we are measuring relative to the baseline, the baseline appears as a solid black line at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe3f69-e5a1-4d05-a193-caefb098e02c",
   "metadata": {
    "id": "aebe3f69-e5a1-4d05-a193-caefb098e02c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ___Cell no. 15___\n",
    "\n",
    "colors = ['purple', 'green'] # colors for the different methods\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.title( \"Accuracy  difference vs. baseline for ML methods\", fontweight ='bold', fontsize =12)\n",
    "plt.xlabel(\"Features\", fontweight ='bold', fontsize =12)\n",
    "plt.ylabel(\"Accuracy  difference from baseline\", fontweight ='bold', fontsize =12)\n",
    "count = 0\n",
    "n = 5\n",
    "space = [] # will be used to label the x-axis\n",
    "tickFeat = [] # will be used to label the x-axis\n",
    "\n",
    "for result, model, color in zip(arr_diff_all, models, colors):\n",
    "    a = np.linspace(n*count, n*(1+count)-2,len(features))\n",
    "    space.extend(a)\n",
    "    tickFeat.extend(result[0])\n",
    "    plt.errorbar( a, result[1], result[2], fmt='o', label =model[1], color = color)\n",
    "    count += 1\n",
    "    \n",
    "plt.plot(np.array(space), np.zeros(len(features)*len(models)), color = 'Black')        \n",
    "\n",
    "# @@@ Label the ticks on the x-axis with the feature set names\n",
    "# @@@ Set reasonable y limits for your plot\n",
    "# @@@ Add a legend that indicates the two types of LR model used.\n",
    "# @@@ Show your plot using plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ce854-ff96-4707-a51a-72edfadb707c",
   "metadata": {
    "id": "076ce854-ff96-4707-a51a-72edfadb707c"
   },
   "source": [
    "**Exercise:** Summarize your results. Are there any estimators that appear to improve over the baseline? Is the improvement statistically significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66bf93-d309-458e-a4a9-ae4f9668ee7e",
   "metadata": {
    "id": "5f66bf93-d309-458e-a4a9-ae4f9668ee7e"
   },
   "source": [
    "### _The END_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa36d82-6f92-4d1c-9a3e-4898fadb35af",
   "metadata": {
    "id": "1aa36d82-6f92-4d1c-9a3e-4898fadb35af"
   },
   "source": [
    "<img src=\"pics/hap.jpg\" width=\"300\" height=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac48610-ac2f-4c81-8726-27a7acde0240",
   "metadata": {
    "id": "4ac48610-ac2f-4c81-8726-27a7acde0240"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
